{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started with Graph Execution\n",
    "link: https://www.tensorflow.org/get_started/get_started_for_beginners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bgolkar/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the train/test feature and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_URL = \"http://download.tensorflow.org/data/iris_training.csv\"\n",
    "TEST_URL = \"http://download.tensorflow.org/data/iris_test.csv\"\n",
    "CSV_COLUMN_NAMES = ['SepalLength', 'SepalWidth', 'PetalLength', 'PetalWidth', 'Species']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(label_name = 'Species'):\n",
    "    \"\"\"Parses the csv file in TRAIN_URL and TEST_URL.\"\"\"\n",
    "\n",
    "    # Create a local copy of the training set.\n",
    "    train_path = tf.keras.utils.get_file(fname = TRAIN_URL.split('/')[-1], origin = TRAIN_URL)\n",
    "    print(train_path)\n",
    "    # train_path now holds the pathname: ~/.keras/datasets/iris_training.csv\n",
    "\n",
    "    # Parse the local CSV file.\n",
    "    train = pd.read_csv(filepath_or_buffer = train_path, names = CSV_COLUMN_NAMES,  # list of column names\n",
    "                        header = 0  # ignore the first row of the CSV file.\n",
    "                       )\n",
    "\n",
    "    # train now holds a pandas DataFrame, which is data structure\n",
    "    # analogous to a table.\n",
    "\n",
    "    # 1. Assign the DataFrame's labels (the right-most column) to train_label.\n",
    "    # 2. Delete (pop) the labels from the DataFrame.\n",
    "    # 3. Assign the remainder of the DataFrame to train_features\n",
    "    train_features, train_label = train, train.pop(label_name)\n",
    "    \n",
    "    # Apply the preceding logic to the test set.\n",
    "    test_path = tf.keras.utils.get_file(TEST_URL.split('/')[-1], TEST_URL)\n",
    "    test = pd.read_csv(test_path, names=CSV_COLUMN_NAMES, header=0)\n",
    "    test_features, test_label = test, test.pop(label_name)\n",
    "\n",
    "    # Return four DataFrames.\n",
    "    return (train_features, train_label), (test_features, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/bgolkar/.keras/datasets/iris_training.csv\n"
     ]
    }
   ],
   "source": [
    "# Call load_data() to parse the CSV file.\n",
    "(train_feature, train_label), (test_feature, test_label) = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120, 4)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SepalLength</th>\n",
       "      <th>SepalWidth</th>\n",
       "      <th>PetalLength</th>\n",
       "      <th>PetalWidth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>3.3</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.9</td>\n",
       "      <td>2.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SepalLength  SepalWidth  PetalLength  PetalWidth\n",
       "0          6.4         2.8          5.6         2.2\n",
       "1          5.0         2.3          3.3         1.0\n",
       "2          4.9         2.5          4.5         1.7\n",
       "3          4.9         3.1          1.5         0.1"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_feature[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = dict(train_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    6.4\n",
       "1    5.0\n",
       "2    4.9\n",
       "Name: SepalLength, dtype: float64"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = a['SepalLength']\n",
    "b[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2.8\n",
       "1    2.3\n",
       "2    2.5\n",
       "Name: SepalWidth, dtype: float64"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = a['SepalWidth']\n",
    "b[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    2\n",
      "1    1\n",
      "2    2\n",
      "3    0\n",
      "Name: Species, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train_label[0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['SepalLength', 'SepalWidth', 'PetalLength', 'PetalWidth'], dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_feature.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RangeIndex(start=0, stop=120, step=1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a tensorflow estimator class\n",
    "A feature column is a data structure that tells your model how to interpret the data in each feature. In the Iris problem, we want the model to interpret the data in each feature as its literal floating-point value; that is, we want the model to interpret an input value like 5.4 as, well, 5.4. However, in other machine learning problems, it is often desirable to interpret data less literally.\n",
    "\n",
    "To define a tensorflow estimator class, feature_columns need to be created according to the dataset. From a code perspective, you build a list of feature_column objects by calling functions from the tf.feature_column module. Each object describes an input to the model. To tell the model to interpret data as a floating-point value, call tf.feature_column.numeric_column.\n",
    "\n",
    "For more on feature columns visit: https://www.tensorflow.org/get_started/feature_columns\n",
    "\n",
    "Here's a snapshot:\n",
    "\n",
    "tf.numeric_column provides optional arguments, calling tf.numeric_column without any arguments, as follows, is a fine way to specify a numerical value with the default data type (tf.float32) as input to your model. Otherwise one could define a feature column as:\n",
    "\n",
    "***numeric_feature_column = tf.feature_column.numeric_column(key = \"SepalLength\", dtype = tf.float64)***\n",
    "\n",
    "By default, a numeric column creates a single value (scalar). Use the shape argument to specify another shape. For example:\n",
    "\n",
    "Represent a 10-element vector in which each cell contains a tf.float32.\n",
    "\n",
    "***vector_feature_column = tf.feature_column.numeric_column(key = \"Bowling\", shape = 10)***\n",
    "\n",
    "Represent a 10x5 matrix in which each cell contains a tf.float32.\n",
    "\n",
    "***matrix_feature_column = tf.feature_column.numeric_column(key = \"MyMatrix\", shape = [10,5])***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature columns for all features\n",
    "my_feature_columns = []\n",
    "for key in train_feature.keys():\n",
    "    \n",
    "    my_feature_columns.append(tf.feature_column.numeric_column(key = key))\n",
    "\n",
    "# Above is equivalent to:\n",
    "# my_feature_columns = [\n",
    "#     tf.feature_column.numeric_column(key='SepalLength'),\n",
    "#     tf.feature_column.numeric_column(key='SepalWidth'),\n",
    "#     tf.feature_column.numeric_column(key='PetalLength'),\n",
    "#     tf.feature_column.numeric_column(key='PetalWidth')\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(my_feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(my_feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[_NumericColumn(key='SepalLength', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " _NumericColumn(key='SepalWidth', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " _NumericColumn(key='PetalLength', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " _NumericColumn(key='PetalWidth', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None)]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_feature_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.estimator.DNNClassifier is a pre-made estimator:\n",
    "\n",
    "The n_classes parameter specifies the number of possible values that the neural network can predict. Since the Iris problem classifies 3 Iris species, we set n_classes to 3.\n",
    "\n",
    "The constructor for tf.Estimator.DNNClassifier takes an optional argument named optimizer, which our sample code chose not to specify. The optimizer controls how the model will train. As you develop more expertise in machine learning, optimizers and learning rate will become very important.\n",
    "\n",
    "Once the estimator class is defined, the next step is to construct the feature and label vectors to train the estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/dt/hdvc539j4zv3fjr1sw5dbqm00000gq/T/tmpk8qyn_jl\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/var/folders/dt/hdvc539j4zv3fjr1sw5dbqm00000gq/T/tmpk8qyn_jl', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x18218a8630>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "classifier = tf.estimator.DNNClassifier(feature_columns = my_feature_columns, hidden_units = [10, 10], n_classes = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.estimator.canned.dnn.DNNClassifier"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.canned.dnn.DNNClassifier at 0x18218a8320>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.data.Dataset.from_tensor_slices((dict(features), labels)) creates the dataset for training/evaluation from the feature and label classes. Note that the features must be put in a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_input_fn(features, labels, batch_size):\n",
    "    \"\"\"An input function for training\"\"\"\n",
    "    # Convert the inputs to a Dataset.\n",
    "    # The dataset API (tf.data.Dataset) is a high-level TensorFlow API for reading data and transforming it into a form that the train method requires.\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))\n",
    "    \n",
    "    # Shuffle, repeat, and batch the examples.\n",
    "    # dataset.shuffle(x): Training works best if the training examples are in random order. To randomize the examples, \n",
    "    # call tf.data.Dataset.shuffle. Setting the buffer_size to a value larger than the number of \n",
    "    # examples (120) ensures that the data will be well shuffled.\n",
    "    # dataset.repeat(): During training, the train method typically processes the examples multiple times. \n",
    "    # Calling the tf.data.Dataset.repeat method without any arguments ensures that the \n",
    "    # train method has an infinite supply of (now shuffled) training set examples.\n",
    "    dataset = dataset.shuffle(1000).repeat().batch(batch_size)\n",
    "    \n",
    "    # Return the dataset.\n",
    "    return dataset.make_one_shot_iterator().get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.data.ops.dataset_ops.TensorSliceDataset'>\n",
      "<TensorSliceDataset shapes: ({SepalLength: (), SepalWidth: (), PetalLength: (), PetalWidth: ()}, ()), types: ({SepalLength: tf.float64, SepalWidth: tf.float64, PetalLength: tf.float64, PetalWidth: tf.float64}, tf.int64)>\n",
      "<class 'tensorflow.python.data.ops.dataset_ops.BatchDataset'>\n",
      "<BatchDataset shapes: ({SepalLength: (?,), SepalWidth: (?,), PetalLength: (?,), PetalWidth: (?,)}, (?,)), types: ({SepalLength: tf.float64, SepalWidth: tf.float64, PetalLength: tf.float64, PetalWidth: tf.float64}, tf.int64)>\n"
     ]
    }
   ],
   "source": [
    "a = train_input_fn(train_feature, train_label, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(a[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.framework.ops.Tensor"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(a[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'PetalLength': <tf.Tensor 'IteratorGetNext_4:0' shape=(?,) dtype=float64>,\n",
       "  'PetalWidth': <tf.Tensor 'IteratorGetNext_4:1' shape=(?,) dtype=float64>,\n",
       "  'SepalLength': <tf.Tensor 'IteratorGetNext_4:2' shape=(?,) dtype=float64>,\n",
       "  'SepalWidth': <tf.Tensor 'IteratorGetNext_4:3' shape=(?,) dtype=float64>},\n",
       " <tf.Tensor 'IteratorGetNext_4:4' shape=(?,) dtype=int64>)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One shot-iterator example\n",
    "A one-shot iterator is the simplest form of iterator, which only supports iterating once through a dataset, with no need for explicit initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.range(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.data.ops.dataset_ops.RangeDataset"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = dataset.shuffle(1000).repeat().batch(10).make_one_shot_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.data.ops.iterator_ops.Iterator"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[39 12 80  2 26  4 92  1 56 65]\n",
      "[95 61 17 91 90 63 44 99 30 69]\n",
      "[70 68 53 64 23 21 97 74 88 15]\n",
      "[72  7 50 52 98 86 59 87  5 29]\n",
      "[48 94 79 25 43 13 34 78 35 22]\n",
      "[60 58 28 76 16 10 96 82 42 71]\n",
      "[41 85 33 54 51 67 40  0 32 84]\n",
      "[62 14 31  6 38 27 57 20 36 89]\n",
      "[55 19 49 11  8 77  3 46 18 24]\n",
      "[47 37 75 83 45 81 93 73 66  9]\n",
      "[63  5 67 68 13 36 16 58 19 76]\n",
      "[94 31 23 18 49 44  9 56 29 25]\n",
      "[70 84 43 26 22 97  3 73 21 42]\n",
      "[64 15 90  6 92 87 14  2 99 53]\n",
      "[95 47 77 12 46 78 59 50 41  7]\n",
      "[65 62 82 71 83 69 60 27 98 81]\n",
      "[ 1 74 10 40  4 45 96 75 33 88]\n",
      "[51 48 85 28 54 17 37 30 39 20]\n",
      "[34 32 93  0  8 11 55 57 66 52]\n",
      "[91 72 35 61 79 38 80 89 86 24]\n"
     ]
    }
   ],
   "source": [
    "# example to understand iterator\n",
    "sess = tf.Session()\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "for i in range(20):\n",
    "    value = sess.run(next_element)\n",
    "    # value type: >class 'numpy.ndarray'>\n",
    "    \n",
    "    print(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lambda wraps arbitrary expression as a Layer object.\n",
    "\n",
    "In the tensorflow estimator class the first argument input_fn is a function that provides input data for training as minibatches. The function should construct and return one of the following:\n",
    "\n",
    "A 'tf.data.Dataset' object: Outputs of Dataset object must be a tuple (features, labels) with same constraints as below.\n",
    "\n",
    "A tuple (features, labels): Where features is a Tensor or a dictionary of string feature name to Tensor and labels is a Tensor or a dictionary of string label name to Tensor. Both features and labels are consumed by model_fn. They should satisfy  the expectation of model_fn from inputs.\n",
    "\n",
    "Please note that input_fn here requires a function with no arguments. As train_input_fn has been defined as train_input_fn(train_feature, train_label, 100), lambda is used to wrap this as a layer object with no arguments. Alternatively, train_input_fn could have been defined with no arguments (set required defaults in the definition and use train_input_fn with no arguments equal to input_fn as:\n",
    "\n",
    "def train_input_fn(features = train_feature, labels = train_label, batch_size = 100):\n",
    "\n",
    "...\n",
    "\n",
    "classifier.train(input_fn = train_input_fn, steps = 1000)\n",
    "\n",
    "...\n",
    "\n",
    "In other words, here we wrap up our input_fn call in a lambda to capture the arguments while providing an input function that takes no arguments, as expected by the Estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.data.ops.dataset_ops.TensorSliceDataset'>\n",
      "<TensorSliceDataset shapes: ({SepalLength: (), SepalWidth: (), PetalLength: (), PetalWidth: ()}, ()), types: ({SepalLength: tf.float64, SepalWidth: tf.float64, PetalLength: tf.float64, PetalWidth: tf.float64}, tf.int64)>\n",
      "<class 'tensorflow.python.data.ops.dataset_ops.BatchDataset'>\n",
      "<BatchDataset shapes: ({SepalLength: (?,), SepalWidth: (?,), PetalLength: (?,), PetalWidth: (?,)}, (?,)), types: ({SepalLength: tf.float64, SepalWidth: tf.float64, PetalLength: tf.float64, PetalWidth: tf.float64}, tf.int64)>\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/dt/hdvc539j4zv3fjr1sw5dbqm00000gq/T/tmpk8qyn_jl/model.ckpt-4000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 4001 into /var/folders/dt/hdvc539j4zv3fjr1sw5dbqm00000gq/T/tmpk8qyn_jl/model.ckpt.\n",
      "INFO:tensorflow:loss = 4.752581, step = 4001\n",
      "INFO:tensorflow:global_step/sec: 577.191\n",
      "INFO:tensorflow:loss = 3.7874985, step = 4101 (0.174 sec)\n",
      "INFO:tensorflow:global_step/sec: 913.408\n",
      "INFO:tensorflow:loss = 4.0978866, step = 4201 (0.110 sec)\n",
      "INFO:tensorflow:global_step/sec: 950.451\n",
      "INFO:tensorflow:loss = 4.468043, step = 4301 (0.105 sec)\n",
      "INFO:tensorflow:global_step/sec: 912.476\n",
      "INFO:tensorflow:loss = 5.4483805, step = 4401 (0.110 sec)\n",
      "INFO:tensorflow:global_step/sec: 926.433\n",
      "INFO:tensorflow:loss = 5.408008, step = 4501 (0.108 sec)\n",
      "INFO:tensorflow:global_step/sec: 916.531\n",
      "INFO:tensorflow:loss = 4.2972827, step = 4601 (0.109 sec)\n",
      "INFO:tensorflow:global_step/sec: 919.719\n",
      "INFO:tensorflow:loss = 2.2741826, step = 4701 (0.109 sec)\n",
      "INFO:tensorflow:global_step/sec: 937.34\n",
      "INFO:tensorflow:loss = 3.751796, step = 4801 (0.107 sec)\n",
      "INFO:tensorflow:global_step/sec: 959.92\n",
      "INFO:tensorflow:loss = 1.8082702, step = 4901 (0.104 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 5000 into /var/folders/dt/hdvc539j4zv3fjr1sw5dbqm00000gq/T/tmpk8qyn_jl/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 1.7793652.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.canned.dnn.DNNClassifier at 0x18218a8320>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.train(input_fn = lambda:train_input_fn(train_feature, train_label, 100), steps = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evluate the estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_input_fn(features, labels = None, batch_size = None):\n",
    "    \"\"\"An input function for evaluation or prediction\"\"\"\n",
    "    if labels is None:\n",
    "        # No labels, use only features.\n",
    "        inputs = dict(features)\n",
    "    else:\n",
    "        inputs = (dict(features), labels)\n",
    "\n",
    "    # Convert inputs to a tf.dataset object.\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(inputs)\n",
    "  \n",
    "    # Batch the examples\n",
    "    assert batch_size is not None, \"batch_size must not be None\"\n",
    "    dataset = dataset.batch(batch_size)\n",
    "\n",
    "    # Return the read end of the pipeline.\n",
    "    return dataset.make_one_shot_iterator().get_next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that classifier.train( input_fn = ..., steps = ...), while classifier.evaluat( input_fn = ...). Unlike our call to the train method, we did not pass the steps argument to evaluate. Our eval_input_fn only yields a single epoch of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-07-09-21:38:56\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/dt/hdvc539j4zv3fjr1sw5dbqm00000gq/T/tmpk8qyn_jl/model.ckpt-5000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-07-09-21:38:56\n",
      "INFO:tensorflow:Saving dict for global step 5000: accuracy = 0.96666664, average_loss = 0.047930546, global_step = 5000, loss = 1.4379164\n",
      "\n",
      "Test set accuracy: 0.967\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model.\n",
    "eval_result = classifier.evaluate(input_fn = lambda:eval_input_fn(test_feature, test_label, 100))\n",
    "\n",
    "print('\\nTest set accuracy: {accuracy:0.3f}\\n'.format(**eval_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict with the estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected = ['Setosa', 'Versicolor', 'Virginica']\n",
    "predict_x = {\n",
    "    'SepalLength': [5.1, 5.9, 6.9],\n",
    "    'SepalWidth': [3.3, 3.0, 3.1],\n",
    "    'PetalLength': [1.7, 4.2, 5.4],\n",
    "    'PetalWidth': [0.5, 1.5, 2.1],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(predict_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall:\n",
    "\n",
    "classifier.train( input_fn = ..., steps = ...)\n",
    "\n",
    "classifier.evaluate( input_fn = ...)\n",
    "\n",
    "classifier.predict( input_fn = ..., batch_size = ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = classifier.predict(input_fn=lambda:eval_input_fn(predict_x,labels = None, batch_size=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predict method returns a python iterable, yielding a dictionary of prediction results for each example. This dictionary contains several keys. The probabilities key holds a list of three floating-point values, each representing the probability that the input example is a particular Iris species.\n",
    "\n",
    "The class_ids key holds a one-element array that identifies the most probable species.\n",
    "\n",
    "The probabilities key holds a list of three floating-point values, each representing the probability that the input example is a particular Iris species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "generator"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/dt/hdvc539j4zv3fjr1sw5dbqm00000gq/T/tmpk8qyn_jl/model.ckpt-5000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "{'logits': array([ 18.27759 ,  10.631209, -23.601736], dtype=float32), 'probabilities': array([9.9952245e-01, 4.7754217e-04, 6.4838418e-19], dtype=float32), 'class_ids': array([0]), 'classes': array([b'0'], dtype=object)} Setosa\n",
      "{'logits': array([-6.087348 ,  3.3560655, -5.6560473], dtype=float32), 'probabilities': array([7.91936181e-05, 9.99798954e-01, 1.21899495e-04], dtype=float32), 'class_ids': array([1]), 'classes': array([b'1'], dtype=object)} Versicolor\n",
      "{'logits': array([-12.167952 ,  -2.9199467,   2.3405356], dtype=float32), 'probabilities': array([4.9750133e-07, 5.1659709e-03, 9.9483347e-01], dtype=float32), 'class_ids': array([2]), 'classes': array([b'2'], dtype=object)} Virginica\n"
     ]
    }
   ],
   "source": [
    "for pred_dict, exp in zip(predictions, expected):\n",
    "    # since predictions is a python iterable, pre_dict is iterating over the iterations\n",
    "    print(pred_dict, exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/dt/hdvc539j4zv3fjr1sw5dbqm00000gq/T/tmp_q3rj0xr/model.ckpt-1000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "\n",
      "Prediction is \"Setosa\" (99.7%), expected \"Setosa\"\n",
      "\n",
      "Prediction is \"Versicolor\" (99.7%), expected \"Versicolor\"\n",
      "\n",
      "Prediction is \"Virginica\" (97.8%), expected \"Virginica\"\n"
     ]
    }
   ],
   "source": [
    "template = ('\\nPrediction is \"{}\" ({:.1f}%), expected \"{}\"')\n",
    "SPECIES = ['Setosa', 'Versicolor', 'Virginica']\n",
    "\n",
    "for pred_dict, expec in zip(predictions, expected):\n",
    "        class_id = pred_dict['class_ids'][0]\n",
    "        probability = pred_dict['probabilities'][class_id]\n",
    "        print(template.format(SPECIES[class_id], 100 * probability, expec))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
